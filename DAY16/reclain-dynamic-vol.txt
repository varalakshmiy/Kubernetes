Reclaim policies
----------------
 Retain(default): When a PV is released from a PVC with a "Retain" reclaim policy, the volume is not automatically deleted. Instead, it remains in the cluster in a released state. The underlying storage is safe, allowing you to manually reclaim or manage the data. adv is data safe.

 --> Default Reclaim policy 
 retain  pv(volume) ---->available
             pv(volume) ---->bound
             pv(volume) ---->released


 Delete: With the "Delete" reclaim policy, when a PVC is deleted, the associated PV is also deleted, along with the underlying storage. This means that all data is lost.

     delete  pv(volume) ---->available
             pv(volume) ---->bound
             pv(volume) ---->failed

 Recycle(Deprecated): The "Recycle" policy allowed for a PV to be scrubbed (data deleted) and then made available again when released. However, this feature has been deprecated in favor of more explicit data management practices. 


Example:
--------
-create pv,mongodbpod,pvc
----------------
apiVersion: v1
kind: PersistentVolume
metadata:
  name: mongodb-pv
spec:
  capacity:
    storage: 1Gi  # Adjust as necessary
  accessModes:
    - ReadWriteMany  # NFS supports this
  nfs:
    server: 172.31.40.109  # Your NFS server IP
    path: /mnt/nfs_share  # Path on your NFS server
  persistentVolumeReclaimPolicy: Delete  # or Delete, depending on your use case
--------------------------------------------------------------------------------------
-when we create a vol first stateof (pv)     -available state
 attache volume pv with pvc second stateof (pv) -bounded state
 dettached(delete) pvc with pv   third stateof (pv)- release







Dynamic volumes with nfs
========================


---
kind: ServiceAccount
apiVersion: v1
metadata:
  name: nfs-pod-provisioner-sa
  namespace: kube-system
---
kind: ClusterRole # Role of kubernetes
apiVersion: rbac.authorization.k8s.io/v1 # auth API
metadata:
  name: nfs-provisioner-clusterRole
rules:
  - apiGroups: [""] # rules on persistentvolumes
    resources: ["persistentvolumes"]
    verbs: ["get", "list", "watch", "create", "delete"]
  - apiGroups: [""]
    resources: ["persistentvolumeclaims"]
    verbs: ["get", "list", "watch", "update"]
  - apiGroups: ["storage.k8s.io"]
    resources: ["storageclasses"]
    verbs: ["get", "list", "watch"]
  - apiGroups: [""]
    resources: ["events"]
    verbs: ["create", "update", "patch"]
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: nfs-provisioner-rolebinding
subjects:
  - kind: ServiceAccount
    name: nfs-pod-provisioner-sa # defined on top of file
    namespace: kube-system
roleRef: # binding cluster role to service account
  kind: ClusterRole
  name: nfs-provisioner-clusterRole # name defined in clusterRole
  apiGroup: rbac.authorization.k8s.io
---
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: nfs-pod-provisioner-otherRoles
  namespace: kube-system
rules:
  - apiGroups: [""]
    resources: ["endpoints"]
    verbs: ["get", "list", "watch", "create", "update", "patch"]
---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: nfs-pod-provisioner-otherRoles
  namespace: kube-system
subjects:
  - kind: ServiceAccount
    name: nfs-pod-provisioner-sa # same as top of the file
    # replace with namespace where provisioner is deployed
    namespace: kube-system
roleRef:
  kind: Role
  name: nfs-pod-provisioner-otherRoles
  apiGroup: rbac.authorization.k8s.io
  
---
kind: Deployment
apiVersion: apps/v1
metadata:
  name: nfs-pod-provisioner
  namespace: kube-system
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nfs-pod-provisioner
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: nfs-pod-provisioner
    spec:
      serviceAccountName: nfs-pod-provisioner-sa # name of service account created in rbac.yaml
      containers:
        - name: nfs-pod-provisioner
          image: rkevin/nfs-subdir-external-provisioner:fix-k8s-1.20
          volumeMounts:
            - name: nfs-provisioner-v
              mountPath: /persistentvolumes
          env:
            - name: PROVISIONER_NAME # do not change
              value: nfs-provisioner # SAME AS PROVISONER NAME VALUE IN STORAGECLASS
            - name: NFS_SERVER # do not change
              value: 172.31.40.109  # Ip of the NFS SERVER
            - name: NFS_PATH # do not change
              value: /mnt/nfs_share # path to nfs directory setup
      volumes:
       - name: nfs-provisioner-v # same as volumemouts name
         nfs:
           server: 172.31.40.109
           path:  /mnt/nfs_share   
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: nfs-storageclass # IMPORTANT pvc needs to mention this name
  annotations:
    storageclass.kubernetes.io/is-default-class: "true"
provisioner: nfs-provisioner # name can be anything
parameters:
  archiveOnDelete: "false"





kubeclt apply -f nfsProviser.yaml



NOTE: to understand the concept delete rs, pvc and pv in the system.





apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mongodb-pvc
  namespace: prod  # This is correct
spec:
  accessModes:
    - ReadWriteMany  # Must match the PV's access mode
  resources:
    requests:
      storage: 1Gi  # Must match or be less than the PV's size
---
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: mongodb
  namespace: prod
spec:
  selector:
    matchLabels:
      app: mongodb
  replicas: 1
  template:
    metadata:
      labels:
        app: mongodb
    spec:
      containers:
      - name: mongocon
        image: mongo
        ports:
        - containerPort: 27017
        env:
        - name: MONGO_INITDB_ROOT_USERNAME
          value: devdb
        - name: MONGO_INITDB_ROOT_PASSWORD
          value: devdb@123
        volumeMounts:
        - name: mongonfsvol
          mountPath: /data/db
      volumes:
      - name: mongonfsvol
        persistentVolumeClaim:
          claimName: mongodb-pvc  # Use the PVC
---
apiVersion: v1
kind: Service
metadata:
  name: mongosvc
  namespace: prod
spec:
  type: ClusterIP
  selector:
    app: mongodb
  ports:
    - port: 27017
      targetPort: 27017
---










apiVersion: apps/v1
kind: Deployment
metadata:
  name: sprinapp
  namespace: test-ns
spec:
  replicas: 2
  selector:
    matchLabels:
      app: springapp
  template:
    metadata:
      labels:
        app: springapp
    spec:
      containers:
      - name: springapp
        image: kkeducation12345/spring-app:1.0.0
        ports:
        - containerPort: 8080
        env:
        - name: MONGO_DB_HOSTNAME
          value: mongosvc
        - name: MONGO_DB_USERNAME
          value: devdb
        - name: MONGO_DB_PASSWORD
          value: devdb@123
        resources:
          requests:
            cpu: 300m
            memory: 256Mi
          limits:
            cpu: 500m
            memory: 512Mi
---
apiVersion: v1
kind: Service
metadata:
  name: springappsvc
  namespace: test-ns
spec:
 type: NodePort
 selector:
   app: springapp
 ports:
 - port: 80
   targetPort: 8080
